# ğŸ§  CNN Classifier for MNIST Handwritten Digits

This project implements a **compact Convolutional Neural Network (CNN)** with approximately **25,000 parameters**, designed and trained on the **MNIST** dataset using **PyTorch**.

---

## ğŸ“š 1. About the MNIST Dataset

**MNIST** (Modified National Institute of Standards and Technology) is a widely used dataset for benchmarking image classification models.

- **Images:** 70,000 grayscale images of handwritten digits (0â€“9)
- **Size:** 28x28 pixels
- **Split:**
  - 60,000 training images
  - 10,000 test images
- **Classes:** 10 (digits 0 through 9)

---

## âš–ï¸ 2. Why Normalize the Data?

Normalization rescales the pixel intensity values to help the network train more effectively.

### ğŸ”¢ Typical normalization for MNIST:
```python
transforms.Normalize((0.1307,), (0.3081,))
```
âœ… **Benefits:**
- Speeds up convergence
- Prevents vanishing/exploding gradients
- Stabilizes learning across layers

---

## ğŸ§ª 3. Data Augmentation

While MNIST is relatively clean, data augmentation helps improve generalization, especially for compact networks.

**Techniques Used:**
```python
transforms.Compose([
    transforms.RandomRotation(10),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
```
âœ… **Advantages:**
- Reduces overfitting
- Improves robustness to real-world variations in handwriting
- Simulates rotation, translation, and distortion

---

## ğŸ§  4. CNN Architecture & Parameters

âœ… **Layer-wise Summary**
![CNN Architecture](images/cnn_architecture.svg)


ğŸ“Š **Parameter Breakdown**

| Layer | Description                  | Parameters |
|-------|------------------------------|------------|
| Conv1 | 1 input â†’ 16 filters (3x3)   | 160       |
| Conv2 | 16 â†’ 32 filters (3x3)        | 4,640     |
| Conv3 | 32 â†’ 40 filters (3x3)        | 11,600    |
| Conv4 | 40 â†’ 24 filters (1x1)        | 984       |
| FC1   | Fully connected (600 â†’ 10)   | 6,010     |
| **Total** |                          | **23,394** âœ… |

---

## ğŸ“ˆ 5. Optimizer Observations

Both SGD and Adam were tested for training the model.

| Optimizer | Convergence Speed | Final Accuracy (MNIST) | Comments                  |
|-----------|-------------------|------------------------|---------------------------|
| SGD      | Slower            | ~96%                   | Needs careful LR tuning  |
| Adam     | Faster            | ~98%                   | Recommended for quick convergence |

ğŸ’¡ **Insight:**  
Adam adapts learning rates per parameter using moment estimates, making it significantly faster than vanilla SGD, especially for smaller models.

### ğŸ§ª Training Configuration

| Parameter       | Value     |
|-----------------|-----------|
| Batch Size      | 512        |
| Epochs          | 20         |
| Loss Function   | nn.NLLLoss() |
| Activation      | ReLU      |
| Final Activation| log_softmax |
| Optimizer       | Adam      |

---

## ğŸ§¾ Requirements

```bash
pip install torch torchvision matplotlib
```

---


## ğŸ¯ Future Work

- Add Dropout for regularization
- Use BatchNorm to stabilize training
- Explore deeper variants with similar param budgets
- Test on Fashion-MNIST or CIFAR-10

---

## ğŸ“Œ Final Notes

âœ… Compact architecture with ~23K parameters  
âœ… Achieves ~98% test accuracy on MNIST with Adam  
![Test Results](images/Test_Results.png)
âœ… Designed for learning, experimentation, and speed  

---

## ğŸ“ License

MIT License

---

## âœï¸ Author

Your Name â€” [@yourgithub](https://github.com/yourgithub)